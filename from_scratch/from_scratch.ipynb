{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to understand the basics of neural networks.\n",
    "\n",
    "We will start with a single neuron and then move on to a simple neural network.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=Model(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        # Convert model parameters to lower precision (float16)\n",
    "        self.half()  # Converts all parameters to half precision (float16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is also in lower precision\n",
    "        x = x.half()\n",
    "        # Pass through the linear layer (single neuron)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = Model()\n",
    "print(f\"{model=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "linear.weight: tensor([[-0.0075]], dtype=torch.float16)\n",
      "linear.bias: tensor([0.5366], dtype=torch.float16)\n",
      "model(torch.tensor([1.0]))=tensor([0.5293], dtype=torch.float16, grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the model parameters\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")\n",
    "print(f\"{model(torch.tensor([1.0]))=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model for the input of 1.0 is a tensor with a single value that is the result of a random initialization of the weights and biases.\n",
    "The output is 0.5290, which is input * weight + bias: 1.0 * -0.0075 + 0.5364 = 0.5289. This is slightly different than 0.5290 b/c the printed values are rounded from the actual float values used.\n",
    "\n",
    "By default, autograd is tracking the operations that are performed on the model.\n",
    "`grad_fn=<ViewBackward0>` tells us that the last operation to create this output was a view operation. Without the view operation the output would have been [[0.5290]] rather than [0.5290]. The `Backward` means that during backgpropagation the Backward function will be called. The `0` suffix means that this will be the first backward call.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 0.529296875,\n",
       " 'loss': 0.2215576171875,\n",
       " 'weight': 0.08660888671875,\n",
       " 'bias': 0.630859375,\n",
       " 'weight_grad': -0.94140625,\n",
       " 'bias_grad': -0.94140625}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a simple training example\n",
    "def train_step(x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()  # This computes gradients for all model parameters\n",
    "    optimizer.step()  # This uses the gradients to update the weights and biases\n",
    "    weight_grad = model.linear.weight.grad.item()\n",
    "    bias_grad = model.linear.bias.grad.item()\n",
    "    # Reset gradients for the next step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Return values for tabulation\n",
    "    return {\n",
    "        'prediction': output.item(),\n",
    "        'loss': loss.item(),\n",
    "        'weight': model.linear.weight.data.item(),\n",
    "        'bias': model.linear.bias.data.item(),\n",
    "        'weight_grad': weight_grad,\n",
    "        'bias_grad': bias_grad\n",
    "    }\n",
    "first_result = train_step(x=torch.tensor([1.0], dtype=torch.float16), y=torch.tensor([1.0], dtype=torch.float16))\n",
    "first_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the loss is ~0.22 which is calculated from the MSE: (1-0.5289)^2 = 0.2219.  \n",
    "The new gradients are the result of the optimizer updating the weights and biases.  \n",
    "For MSELoss, the gradient is ∂Loss/∂output = 2 * (output - target) / n, which is 2 * (0.5289 - 1) / 1 = -0.9422 for the weight and bias (not shown). This is then used to update the weights and biases from above:  \n",
    "new weight = old_weight - learning_rate * gradient_weight = -0.0075 - 0.01 * -0.9422 = 0.0019  \n",
    "new bias = old_bias - learning_rate * gradient_bias = 0.5364 - 0.01 * -0.9422 = 0.5458\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>prediction</th>\n",
       "      <th>loss</th>\n",
       "      <th>weight</th>\n",
       "      <th>bias</th>\n",
       "      <th>weight_grad</th>\n",
       "      <th>bias_grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.529297</td>\n",
       "      <td>2.215576e-01</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.630859</td>\n",
       "      <td>-0.941406</td>\n",
       "      <td>-0.941406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.717285</td>\n",
       "      <td>7.995605e-02</td>\n",
       "      <td>0.143066</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>-0.565430</td>\n",
       "      <td>-0.565430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.830566</td>\n",
       "      <td>2.870178e-02</td>\n",
       "      <td>0.177002</td>\n",
       "      <td>0.721191</td>\n",
       "      <td>-0.338867</td>\n",
       "      <td>-0.338867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1.031494e-02</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>-0.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>3.725052e-03</td>\n",
       "      <td>0.209473</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>-0.122070</td>\n",
       "      <td>-0.122070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.963379</td>\n",
       "      <td>1.340866e-03</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.761230</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.073242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>4.827976e-04</td>\n",
       "      <td>0.221191</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>-0.043945</td>\n",
       "      <td>-0.043945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>1.738071e-04</td>\n",
       "      <td>0.223877</td>\n",
       "      <td>0.768066</td>\n",
       "      <td>-0.026367</td>\n",
       "      <td>-0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>6.103516e-05</td>\n",
       "      <td>0.225464</td>\n",
       "      <td>0.769531</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.995117</td>\n",
       "      <td>2.384186e-05</td>\n",
       "      <td>0.226440</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>8.583069e-06</td>\n",
       "      <td>0.227051</td>\n",
       "      <td>0.770996</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>3.814697e-06</td>\n",
       "      <td>0.227417</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.003906</td>\n",
       "      <td>-0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.227661</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>-0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.227905</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>-0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>2.384186e-07</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>2.384186e-07</td>\n",
       "      <td>0.228149</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>2.384186e-07</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration  prediction          loss    weight      bias  weight_grad  \\\n",
       "0           0    0.529297  2.215576e-01  0.086609  0.630859    -0.941406   \n",
       "1           1    0.717285  7.995605e-02  0.143066  0.687500    -0.565430   \n",
       "2           2    0.830566  2.870178e-02  0.177002  0.721191    -0.338867   \n",
       "3           3    0.898438  1.031494e-02  0.197266  0.741699    -0.203125   \n",
       "4           4    0.938965  3.725052e-03  0.209473  0.753906    -0.122070   \n",
       "5           5    0.963379  1.340866e-03  0.216797  0.761230    -0.073242   \n",
       "6           6    0.978027  4.827976e-04  0.221191  0.765625    -0.043945   \n",
       "7           7    0.986816  1.738071e-04  0.223877  0.768066    -0.026367   \n",
       "8           8    0.992188  6.103516e-05  0.225464  0.769531    -0.015625   \n",
       "9           9    0.995117  2.384186e-05  0.226440  0.770508    -0.009766   \n",
       "10         10    0.997070  8.583069e-06  0.227051  0.770996    -0.005859   \n",
       "11         11    0.998047  3.814697e-06  0.227417  0.771484    -0.003906   \n",
       "12         12    0.999023  9.536743e-07  0.227661  0.771484    -0.001953   \n",
       "13         13    0.999023  9.536743e-07  0.227905  0.771484    -0.001953   \n",
       "14         14    0.999512  2.384186e-07  0.228027  0.771484    -0.000977   \n",
       "15         15    0.999512  2.384186e-07  0.228149  0.771484    -0.000977   \n",
       "16         16    0.999512  2.384186e-07  0.228271  0.771484    -0.000977   \n",
       "17         17    1.000000  0.000000e+00  0.228271  0.771484     0.000000   \n",
       "18         18    1.000000  0.000000e+00  0.228271  0.771484     0.000000   \n",
       "19         19    1.000000  0.000000e+00  0.228271  0.771484     0.000000   \n",
       "\n",
       "    bias_grad  \n",
       "0   -0.941406  \n",
       "1   -0.565430  \n",
       "2   -0.338867  \n",
       "3   -0.203125  \n",
       "4   -0.122070  \n",
       "5   -0.073242  \n",
       "6   -0.043945  \n",
       "7   -0.026367  \n",
       "8   -0.015625  \n",
       "9   -0.009766  \n",
       "10  -0.005859  \n",
       "11  -0.003906  \n",
       "12  -0.001953  \n",
       "13  -0.001953  \n",
       "14  -0.000977  \n",
       "15  -0.000977  \n",
       "16  -0.000977  \n",
       "17   0.000000  \n",
       "18   0.000000  \n",
       "19   0.000000  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame from the first training result\n",
    "results_df = pd.DataFrame([first_result])\n",
    "# Add an iteration column and set it to 0\n",
    "results_df.insert(0, 'iteration', 0)\n",
    "\n",
    "for i in range(1, 20):\n",
    "    result = train_step(x=torch.tensor([1.0], dtype=torch.float16), y=torch.tensor([1.0], dtype=torch.float16))\n",
    "    # Create a new row with the current iteration and result\n",
    "    new_row = pd.DataFrame([{**{'iteration': i}, **result}])\n",
    "    # Append the new row to results_df\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loss is zero, the gradients also become zero because they're calculated from the loss. At this point the weights and biases are not updated.\n",
    "\n",
    "Side note: If torch.float32 is used, it will take longer to converge because it takes longer for the loss to reach zero at float32 precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
