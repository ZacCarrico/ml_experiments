{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifying User Posts Using Machine Learning\n",
    "\n",
    "The purpose of this of this project is to explore machine learning algorithms for classifying newsroom posts into predefined categories. The motivation for exploring this is to better understand why certain algorithms work well for topic classification. The four categories are 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', and user posts are from a collection avialable from SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# the %%capture prevents the DeprecationWarning message for one of the modules\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is from sklearn.datasets fetch_20newsgroups. Metadata is stripped out, and the each newsgroup is split into train and tests sets. The test set is further split to provide a development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training label shape:', (2034,))\n",
      "('test label shape:', (677,))\n",
      "('dev label shape:', (676,))\n",
      "('labels names:', ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'])\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "n_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[int(n_test/2):], newsgroups_test.target[int(n_test/2):]\n",
    "dev_data, dev_labels = newsgroups_test.data[:int(n_test/2)], newsgroups_test.target[:int(n_test/2)]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's look at a few of these posts. Label 0-3 are 'alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "category: 1 \n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych \n",
      "======================\n",
      "\n",
      "\n",
      "======================\n",
      "category: 3 \n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries. \n",
      "======================\n",
      "\n",
      "\n",
      "======================\n",
      "category: 2 \n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      "  \n",
      "======================\n",
      "\n",
      "\n",
      "======================\n",
      "category: 0 \n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question. \n",
      "======================\n",
      "\n",
      "\n",
      "======================\n",
      "category: 2 \n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go? \n",
      "======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    print \"\\n======================\\ncategory:\", train_labels[x],\"\\n\", \\\n",
    "         train_data[x], \\\n",
    "         \"\\n======================\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These posts have a lot of variety in terms of their content and formatting. This makes for an interesting classification challenge!\n",
    "\n",
    "To turn the posts into something machine intelligible we will tokenize the words and create feature vectors. To do this we will use Sci-kit-learn's CountVectorizer. CountVectorizer's fit_transform function generates a tuple and int for each word. Element 1 of the tuple is the document index and element 2 is the word token. The integer that follows the tuple is the number of counts of that tuple. In other words it is the number of occurences of that word in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the user comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26879)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This tells us that there are 2034 posts and 26879 features. Most of these features are words and numbers, but others are strings of unknown meaning. For example '000062david42' is a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The below creates a dataframe for which each column is a feature of the training set. The values in each column are the number of occurences of that feature in that post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26879)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000005102000</th>\n",
       "      <th>000062david42</th>\n",
       "      <th>0001</th>\n",
       "      <th>000100255pixel</th>\n",
       "      <th>00041032</th>\n",
       "      <th>...</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zus</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zwaartepunten</th>\n",
       "      <th>zwak</th>\n",
       "      <th>zwakke</th>\n",
       "      <th>zware</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2034 rows  26879 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0000  00000  000000  000005102000  000062david42  0001  \\\n",
       "0      0    0     0      0       0             0              0     0   \n",
       "1      0    0     0      0       0             0              0     0   \n",
       "2      0    0     0      0       0             0              0     0   \n",
       "3      0    0     0      0       0             0              0     0   \n",
       "4      0    0     0      0       0             0              0     0   \n",
       "5      0    0     0      0       0             0              0     0   \n",
       "6      0    0     0      0       0             0              0     0   \n",
       "7      0    0     0      0       0             0              0     0   \n",
       "8      0    0     0      0       0             0              0     0   \n",
       "9      0    0     0      0       0             0              0     0   \n",
       "10     0    0     0      0       0             0              0     0   \n",
       "11     0    0     0      0       0             0              0     0   \n",
       "12     0    0     0      0       0             0              0     0   \n",
       "13     0    0     0      0       0             0              0     0   \n",
       "14     0    0     0      0       0             0              0     0   \n",
       "15     0    0     0      0       0             0              0     0   \n",
       "16     0    0     0      0       0             0              0     0   \n",
       "17     0    0     0      0       0             0              0     0   \n",
       "18     0    0     0      0       0             0              0     0   \n",
       "19     0    0     0      0       0             0              0     0   \n",
       "20     0    0     0      0       0             0              0     0   \n",
       "21     0    0     0      0       0             0              0     0   \n",
       "22     0    0     0      0       0             0              0     0   \n",
       "23     0    2     0      0       0             0              0     0   \n",
       "24     0    0     0      0       0             0              0     0   \n",
       "25     0    0     0      0       0             0              0     0   \n",
       "26     0    0     0      0       0             0              0     0   \n",
       "27     0    0     0      0       0             0              0     0   \n",
       "28     0    0     0      0       0             0              0     0   \n",
       "29     0    0     0      0       0             0              0     0   \n",
       "...   ..  ...   ...    ...     ...           ...            ...   ...   \n",
       "2004   0    0     0      0       0             0              0     0   \n",
       "2005   0    0     0      0       0             0              0     0   \n",
       "2006   0    0     0      0       0             0              0     0   \n",
       "2007   0    0     0      0       0             0              0     0   \n",
       "2008   0    0     0      0       0             0              0     0   \n",
       "2009   0    0     0      0       0             0              0     0   \n",
       "2010   0    0     0      0       0             0              0     0   \n",
       "2011   0    2     0      0       0             0              0     0   \n",
       "2012   0    0     0      0       0             0              0     0   \n",
       "2013   0    0     0      0       0             0              0     0   \n",
       "2014   0    0     0      0       0             0              0     0   \n",
       "2015   0    0     0      0       0             0              0     0   \n",
       "2016   0    0     0      0       0             0              0     0   \n",
       "2017   0    0     0      0       0             0              0     0   \n",
       "2018   0    0     0      0       0             0              0     0   \n",
       "2019   0    0     0      0       0             0              0     0   \n",
       "2020   1    0     0      0       0             0              0     0   \n",
       "2021   0    0     0      0       0             0              0     0   \n",
       "2022   0    0     0      0       0             0              0     0   \n",
       "2023   0    0     0      0       0             0              0     0   \n",
       "2024   0    0     0      0       0             0              0     0   \n",
       "2025   0    0     0      0       0             0              0     0   \n",
       "2026   0    0     0      0       0             0              0     0   \n",
       "2027   0    0     0      0       0             0              0     0   \n",
       "2028   0    0     0      0       0             0              0     0   \n",
       "2029   0    0     0      0       0             0              0     0   \n",
       "2030   0    0     0      0       0             0              0     0   \n",
       "2031   0    0     0      0       0             0              0     0   \n",
       "2032   0    0     0      0       0             0              0     0   \n",
       "2033   0    0     0      0       0             0              0     0   \n",
       "\n",
       "      000100255pixel  00041032  ...    zurich  zurvanism  zus  zvi  \\\n",
       "0                  0         0  ...         0          0    0    0   \n",
       "1                  0         0  ...         0          0    0    0   \n",
       "2                  0         0  ...         0          0    0    0   \n",
       "3                  0         0  ...         0          0    0    0   \n",
       "4                  0         0  ...         0          0    0    0   \n",
       "5                  0         0  ...         0          0    0    0   \n",
       "6                  0         0  ...         0          0    0    0   \n",
       "7                  0         0  ...         0          0    0    0   \n",
       "8                  0         0  ...         0          0    0    0   \n",
       "9                  0         0  ...         0          0    0    0   \n",
       "10                 0         0  ...         0          0    0    0   \n",
       "11                 0         0  ...         0          0    0    0   \n",
       "12                 0         0  ...         0          0    0    0   \n",
       "13                 0         0  ...         0          0    0    0   \n",
       "14                 0         0  ...         0          0    0    0   \n",
       "15                 0         0  ...         0          0    0    0   \n",
       "16                 0         0  ...         0          0    0    0   \n",
       "17                 0         0  ...         0          0    0    0   \n",
       "18                 0         0  ...         0          0    0    0   \n",
       "19                 0         0  ...         0          0    0    0   \n",
       "20                 0         0  ...         0          0    0    0   \n",
       "21                 0         0  ...         0          0    0    0   \n",
       "22                 0         0  ...         0          0    0    0   \n",
       "23                 0         0  ...         0          0    0    0   \n",
       "24                 0         0  ...         0          0    0    0   \n",
       "25                 0         0  ...         0          0    0    0   \n",
       "26                 0         0  ...         0          0    0    0   \n",
       "27                 0         0  ...         0          0    0    0   \n",
       "28                 0         0  ...         0          0    0    0   \n",
       "29                 0         0  ...         0          0    0    0   \n",
       "...              ...       ...  ...       ...        ...  ...  ...   \n",
       "2004               0         0  ...         0          0    0    0   \n",
       "2005               0         0  ...         0          0    0    0   \n",
       "2006               0         0  ...         0          0    0    0   \n",
       "2007               0         0  ...         0          0    0    0   \n",
       "2008               0         0  ...         0          0    0    0   \n",
       "2009               0         0  ...         0          0    0    0   \n",
       "2010               0         0  ...         0          0    0    0   \n",
       "2011               0         0  ...         0          0    0    0   \n",
       "2012               0         0  ...         0          0    0    0   \n",
       "2013               0         0  ...         0          0    0    0   \n",
       "2014               0         0  ...         0          0    0    0   \n",
       "2015               0         0  ...         0          0    0    0   \n",
       "2016               0         0  ...         0          0    0    0   \n",
       "2017               0         0  ...         0          0    0    0   \n",
       "2018               0         0  ...         0          0    0    0   \n",
       "2019               0         0  ...         0          0    0    0   \n",
       "2020               0         0  ...         0          0    0    0   \n",
       "2021               0         0  ...         0          0    0    0   \n",
       "2022               0         0  ...         0          0    0    0   \n",
       "2023               0         0  ...         0          0    0    0   \n",
       "2024               0         0  ...         0          0    0    0   \n",
       "2025               0         0  ...         0          0    0    0   \n",
       "2026               0         0  ...         0          0    0    0   \n",
       "2027               0         0  ...         0          0    0    0   \n",
       "2028               0         0  ...         0          0    0    0   \n",
       "2029               0         0  ...         0          0    0    0   \n",
       "2030               0         0  ...         0          0    0    0   \n",
       "2031               0         0  ...         0          0    0    0   \n",
       "2032               0         0  ...         0          0    0    0   \n",
       "2033               0         0  ...         0          0    0    0   \n",
       "\n",
       "      zwaartepunten  zwak  zwakke  zware  zwarte  zyxel  \n",
       "0                 0     0       0      0       0      0  \n",
       "1                 0     0       0      0       0      0  \n",
       "2                 0     0       0      0       0      0  \n",
       "3                 0     0       0      0       0      0  \n",
       "4                 0     0       0      0       0      0  \n",
       "5                 0     0       0      0       0      0  \n",
       "6                 0     0       0      0       0      0  \n",
       "7                 0     0       0      0       0      0  \n",
       "8                 0     0       0      0       0      0  \n",
       "9                 0     0       0      0       0      0  \n",
       "10                0     0       0      0       0      0  \n",
       "11                0     0       0      0       0      0  \n",
       "12                0     0       0      0       0      0  \n",
       "13                0     0       0      0       0      0  \n",
       "14                0     0       0      0       0      0  \n",
       "15                0     0       0      0       0      0  \n",
       "16                0     0       0      0       0      0  \n",
       "17                0     0       0      0       0      0  \n",
       "18                0     0       0      0       0      0  \n",
       "19                0     0       0      0       0      0  \n",
       "20                0     0       0      0       0      0  \n",
       "21                0     0       0      0       0      0  \n",
       "22                0     0       0      0       0      0  \n",
       "23                0     0       0      0       0      0  \n",
       "24                0     0       0      0       0      0  \n",
       "25                0     0       0      0       0      0  \n",
       "26                0     0       0      0       0      0  \n",
       "27                0     0       0      0       0      0  \n",
       "28                0     0       0      0       0      0  \n",
       "29                0     0       0      0       0      0  \n",
       "...             ...   ...     ...    ...     ...    ...  \n",
       "2004              0     0       0      0       0      0  \n",
       "2005              0     0       0      0       0      0  \n",
       "2006              0     0       0      0       0      0  \n",
       "2007              0     0       0      0       0      0  \n",
       "2008              0     0       0      0       0      0  \n",
       "2009              0     0       0      0       0      0  \n",
       "2010              0     0       0      0       0      0  \n",
       "2011              0     0       0      0       0      0  \n",
       "2012              0     0       0      0       0      0  \n",
       "2013              0     0       0      0       0      0  \n",
       "2014              0     0       0      0       0      0  \n",
       "2015              0     0       0      0       0      0  \n",
       "2016              0     0       0      0       0      0  \n",
       "2017              0     0       0      0       0      0  \n",
       "2018              0     0       0      0       0      0  \n",
       "2019              0     0       0      0       0      0  \n",
       "2020              0     0       0      0       0      0  \n",
       "2021              0     0       0      0       0      0  \n",
       "2022              0     0       0      0       0      0  \n",
       "2023              0     0       0      0       0      0  \n",
       "2024              0     0       0      0       0      0  \n",
       "2025              0     0       0      0       0      0  \n",
       "2026              0     0       0      0       0      0  \n",
       "2027              0     0       0      0       0      0  \n",
       "2028              0     0       0      0       0      0  \n",
       "2029              0     0       0      0       0      0  \n",
       "2030              0     0       0      0       0      0  \n",
       "2031              0     0       0      0       0      0  \n",
       "2032              0     0       0      0       0      0  \n",
       "2033              0     0       0      0       0      0  \n",
       "\n",
       "[2034 rows x 26879 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(vectorizer.fit_transform(train_data).todense(), columns = vectorizer.get_feature_names())\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is clearly very sparse data -- look at all those zeros!\n",
    "What fraction of the matrix are non-zero values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Out of curiosity, let's look at the first and last words in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zeroth and last feature strings are: 00 and zyxel.\n"
     ]
    }
   ],
   "source": [
    "print \"The zeroth and last feature strings are:\", \\\n",
    "      vectorizer.get_feature_names()[0], \\\n",
    "      \"and\", \\\n",
    "     vectorizer.get_feature_names()[len(vectorizer.get_feature_names()) -1] + \".\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Zyxel is a communications company, and suggests that attempts to remove non-English words would be a bad idea since many names/identifiers would be lost, which would likely decrease the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data set has 26879 unique features.\n",
      "The development data set has 16246 unique features.\n",
      "And the two share 12219 unique features\n"
     ]
    }
   ],
   "source": [
    "# calculating the number of unique tokens (ie words) in the training and dev sets\n",
    "train_vectorizer = CountVectorizer()\n",
    "X_train = train_vectorizer.fit_transform(train_data)\n",
    "\n",
    "dev_vectorizer = CountVectorizer()\n",
    "X_dev = dev_vectorizer.fit_transform(dev_data)\n",
    "\n",
    "print \"The training data set has\", len(set(train_vectorizer.vocabulary_)), \"unique features.\"\n",
    "print \"The development data set has\", len(set(dev_vectorizer.vocabulary_)), \"unique features.\"\n",
    "print \"And the two share\", len(set(dev_vectorizer.vocabulary_.keys()) & set(train_vectorizer.vocabulary_.keys())), \"unique features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels_dict = {0 : 'alt.atheism', \n",
    "               1 : 'comp.graphics',\n",
    "               2 : 'sci.space',\n",
    "               3 : 'talk.religion.misc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression for classifying user comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_vectorizer(ngram):\n",
    "    # instantiating CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # fitting and transforming training data\n",
    "    X_train = vectorizer.fit_transform(train_data)\n",
    "\n",
    "    # this was the tricky part for me because I didn't know that I should use the same\n",
    "    # CountVectorizer and just .transform the dev_data to get it to work\n",
    "    X_dev = vectorizer.transform(dev_data)\n",
    "\n",
    "    # instantiating a logistic regression model\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X=X_train, y=train_labels)\n",
    "    \n",
    "    n_top = 3\n",
    "    words = []\n",
    "    top_ns = []\n",
    "    pred_category = []\n",
    "    df = pd.DataFrame()\n",
    "    for i in set(train_labels):\n",
    "        # gathering the coefficients\n",
    "        top_n = np.argsort(lr.coef_[i])[-n_top:]\n",
    "        top_ns.extend(list(top_n))\n",
    "        pred_category.extend([labels_dict[i]]*n_top)\n",
    "        print 'mean and stddev of the coefficients for category', i, \":\", np.mean(lr.coef_[i]), \",\", np.std(lr.coef_[i])\n",
    "    for j in top_ns:\n",
    "        # gathering the words with the largest coeficients\n",
    "        words.append(vectorizer.get_feature_names()[j]) # the feature names are the tokenized words\n",
    "    df['word(s)'] = words\n",
    "    df['pred_category'] = pred_category\n",
    "    for i in set(train_labels):\n",
    "        df[labels_dict[i] + \"_coef\"] = lr.coef_[i][top_ns]\n",
    "    \n",
    "    print \"classification report for ngram_range\", ngram, \"\\n\", \\\n",
    "        classification_report(y_true = dev_labels, y_pred = lr.predict(X_dev))\n",
    "    display(df)\n",
    "    print \"========\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There's a lot to talk about in the below data. First, this shows that the monogram approach exceeds that of the bigram or trigram with respect to precision and recall. The reason for this is probably due to the most relevant topic identifiers being a single word, rather than two or three word phrases.\n",
    "Secondly, notice that the trigram has some seemingly meaningless phrases, indicating it's a poor approach.\n",
    "\n",
    "Now let's talk about the coefficients shown for each category. The largest coefficients for each predicted category were identified, and the word(s) associated with this coefficient extracted. Notice that the coefficients are larger for the monogram than for the bigram and trigram. This is also apparent in the standard deviations -- for the monogram the stddev is several-fold greater than the bi and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and stddev of the coefficients for category 0 : -0.002303413677017988 , 0.0787821264351687\n",
      "mean and stddev of the coefficients for category 1 : -0.000720433179118076 , 0.06977261275787278\n",
      "mean and stddev of the coefficients for category 2 : 0.00023755130931552037 , 0.07664199358312632\n",
      "mean and stddev of the coefficients for category 3 : -0.0018505818886303236 , 0.07360915128836859\n",
      "classification report for ngram_range (1, 1) \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.56      0.59       165\n",
      "          1       0.74      0.88      0.81       185\n",
      "          2       0.77      0.76      0.77       199\n",
      "          3       0.59      0.53      0.56       127\n",
      "\n",
      "avg / total       0.69      0.70      0.69       676\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>pred_category</th>\n",
       "      <th>alt.atheism_coef</th>\n",
       "      <th>comp.graphics_coef</th>\n",
       "      <th>sci.space_coef</th>\n",
       "      <th>talk.religion.misc_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bobby</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.989956</td>\n",
       "      <td>-0.221093</td>\n",
       "      <td>-0.340860</td>\n",
       "      <td>-0.463411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atheists</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1.030696</td>\n",
       "      <td>-0.096547</td>\n",
       "      <td>-0.318807</td>\n",
       "      <td>-0.835035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deletion</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1.125056</td>\n",
       "      <td>-0.397583</td>\n",
       "      <td>-0.419910</td>\n",
       "      <td>-0.394396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.334684</td>\n",
       "      <td>1.266316</td>\n",
       "      <td>-0.806165</td>\n",
       "      <td>-0.626966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.582814</td>\n",
       "      <td>1.345943</td>\n",
       "      <td>-0.825270</td>\n",
       "      <td>-0.467288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.758397</td>\n",
       "      <td>1.936768</td>\n",
       "      <td>-1.336505</td>\n",
       "      <td>-0.763043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nasa</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.572388</td>\n",
       "      <td>-0.478139</td>\n",
       "      <td>1.011602</td>\n",
       "      <td>-0.467679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>orbit</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.413948</td>\n",
       "      <td>-0.671164</td>\n",
       "      <td>1.224774</td>\n",
       "      <td>-0.629579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-1.260178</td>\n",
       "      <td>-1.316531</td>\n",
       "      <td>2.161801</td>\n",
       "      <td>-1.170735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blood</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.533300</td>\n",
       "      <td>-0.106914</td>\n",
       "      <td>-0.316273</td>\n",
       "      <td>1.054174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>christian</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.607864</td>\n",
       "      <td>-0.418361</td>\n",
       "      <td>-0.270274</td>\n",
       "      <td>1.117683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>christians</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.740208</td>\n",
       "      <td>-0.409151</td>\n",
       "      <td>-0.525317</td>\n",
       "      <td>1.148107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word(s)       pred_category  alt.atheism_coef  comp.graphics_coef  \\\n",
       "0        bobby         alt.atheism          0.989956           -0.221093   \n",
       "1     atheists         alt.atheism          1.030696           -0.096547   \n",
       "2     deletion         alt.atheism          1.125056           -0.397583   \n",
       "3         file       comp.graphics         -0.334684            1.266316   \n",
       "4        image       comp.graphics         -0.582814            1.345943   \n",
       "5     graphics       comp.graphics         -0.758397            1.936768   \n",
       "6         nasa           sci.space         -0.572388           -0.478139   \n",
       "7        orbit           sci.space         -0.413948           -0.671164   \n",
       "8        space           sci.space         -1.260178           -1.316531   \n",
       "9        blood  talk.religion.misc         -0.533300           -0.106914   \n",
       "10   christian  talk.religion.misc         -0.607864           -0.418361   \n",
       "11  christians  talk.religion.misc         -0.740208           -0.409151   \n",
       "\n",
       "    sci.space_coef  talk.religion.misc_coef  \n",
       "0        -0.340860                -0.463411  \n",
       "1        -0.318807                -0.835035  \n",
       "2        -0.419910                -0.394396  \n",
       "3        -0.806165                -0.626966  \n",
       "4        -0.825270                -0.467288  \n",
       "5        -1.336505                -0.763043  \n",
       "6         1.011602                -0.467679  \n",
       "7         1.224774                -0.629579  \n",
       "8         2.161801                -1.170735  \n",
       "9        -0.316273                 1.054174  \n",
       "10       -0.270274                 1.117683  \n",
       "11       -0.525317                 1.148107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "\n",
      "mean and stddev of the coefficients for category 0 : -0.0011705295046437229 , 0.02955175847815318\n",
      "mean and stddev of the coefficients for category 1 : -0.000361007815361034 , 0.030448851343429704\n",
      "mean and stddev of the coefficients for category 2 : -0.0005378179697379003 , 0.031069249422986066\n",
      "mean and stddev of the coefficients for category 3 : -0.0013159588831620685 , 0.0282077021772131\n",
      "classification report for ngram_range (2, 2) \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.55      0.57       165\n",
      "          1       0.62      0.82      0.70       185\n",
      "          2       0.65      0.71      0.68       199\n",
      "          3       0.65      0.32      0.43       127\n",
      "\n",
      "avg / total       0.63      0.63      0.61       676\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>pred_category</th>\n",
       "      <th>alt.atheism_coef</th>\n",
       "      <th>comp.graphics_coef</th>\n",
       "      <th>sci.space_coef</th>\n",
       "      <th>talk.religion.misc_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cheers kent</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.649144</td>\n",
       "      <td>-0.882288</td>\n",
       "      <td>-0.821936</td>\n",
       "      <td>0.601477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was just</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.677623</td>\n",
       "      <td>-0.192925</td>\n",
       "      <td>-0.197945</td>\n",
       "      <td>-0.301982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim that</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.771680</td>\n",
       "      <td>-0.257707</td>\n",
       "      <td>-0.352081</td>\n",
       "      <td>-0.200703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in advance</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.544996</td>\n",
       "      <td>0.972282</td>\n",
       "      <td>-0.531010</td>\n",
       "      <td>-0.507304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.379829</td>\n",
       "      <td>1.037308</td>\n",
       "      <td>-0.470812</td>\n",
       "      <td>-0.396651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>looking for</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.755597</td>\n",
       "      <td>1.319679</td>\n",
       "      <td>-0.613692</td>\n",
       "      <td>-0.699852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sci space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.317719</td>\n",
       "      <td>-0.388504</td>\n",
       "      <td>0.738842</td>\n",
       "      <td>-0.274843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the moon</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.404810</td>\n",
       "      <td>-0.576411</td>\n",
       "      <td>0.951758</td>\n",
       "      <td>-0.240626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.314117</td>\n",
       "      <td>-0.645741</td>\n",
       "      <td>1.030145</td>\n",
       "      <td>-0.324507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>compuserve com</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.132647</td>\n",
       "      <td>-0.211820</td>\n",
       "      <td>-0.200880</td>\n",
       "      <td>0.701502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>such lunacy</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.149092</td>\n",
       "      <td>-0.216348</td>\n",
       "      <td>-0.190673</td>\n",
       "      <td>0.706587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ignorance is</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.227889</td>\n",
       "      <td>-0.253128</td>\n",
       "      <td>-0.211358</td>\n",
       "      <td>0.745662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word(s)       pred_category  alt.atheism_coef  comp.graphics_coef  \\\n",
       "0      cheers kent         alt.atheism          0.649144           -0.882288   \n",
       "1         was just         alt.atheism          0.677623           -0.192925   \n",
       "2       claim that         alt.atheism          0.771680           -0.257707   \n",
       "3       in advance       comp.graphics         -0.544996            0.972282   \n",
       "4    comp graphics       comp.graphics         -0.379829            1.037308   \n",
       "5      looking for       comp.graphics         -0.755597            1.319679   \n",
       "6        sci space           sci.space         -0.317719           -0.388504   \n",
       "7         the moon           sci.space         -0.404810           -0.576411   \n",
       "8        the space           sci.space         -0.314117           -0.645741   \n",
       "9   compuserve com  talk.religion.misc         -0.132647           -0.211820   \n",
       "10     such lunacy  talk.religion.misc         -0.149092           -0.216348   \n",
       "11    ignorance is  talk.religion.misc         -0.227889           -0.253128   \n",
       "\n",
       "    sci.space_coef  talk.religion.misc_coef  \n",
       "0        -0.821936                 0.601477  \n",
       "1        -0.197945                -0.301982  \n",
       "2        -0.352081                -0.200703  \n",
       "3        -0.531010                -0.507304  \n",
       "4        -0.470812                -0.396651  \n",
       "5        -0.613692                -0.699852  \n",
       "6         0.738842                -0.274843  \n",
       "7         0.951758                -0.240626  \n",
       "8         1.030145                -0.324507  \n",
       "9        -0.200880                 0.701502  \n",
       "10       -0.190673                 0.706587  \n",
       "11       -0.211358                 0.745662  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "\n",
      "mean and stddev of the coefficients for category 0 : -0.001979061104483735 , 0.024369227543747256\n",
      "mean and stddev of the coefficients for category 1 : -0.0022930858649384644 , 0.026199600746962058\n",
      "mean and stddev of the coefficients for category 2 : -0.0009958035133713098 , 0.025793266352965093\n",
      "mean and stddev of the coefficients for category 3 : -0.0025432667441444754 , 0.02291327590420167\n",
      "classification report for ngram_range (3, 3) \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.34      0.43       165\n",
      "          1       0.43      0.91      0.58       185\n",
      "          2       0.63      0.49      0.55       199\n",
      "          3       0.69      0.17      0.28       127\n",
      "\n",
      "avg / total       0.58      0.51      0.48       676\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>pred_category</th>\n",
       "      <th>alt.atheism_coef</th>\n",
       "      <th>comp.graphics_coef</th>\n",
       "      <th>sci.space_coef</th>\n",
       "      <th>talk.religion.misc_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grow up childish</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.626089</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>-0.155396</td>\n",
       "      <td>-0.114554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look up irony</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.626089</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>-0.155396</td>\n",
       "      <td>-0.114554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up childish propagandist</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.626089</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>-0.155396</td>\n",
       "      <td>-0.114554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agree with you</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.241578</td>\n",
       "      <td>0.666488</td>\n",
       "      <td>-0.378151</td>\n",
       "      <td>0.129854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>am looking for</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.643466</td>\n",
       "      <td>1.116126</td>\n",
       "      <td>-0.426317</td>\n",
       "      <td>-0.622128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thanks in advance</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>-0.699582</td>\n",
       "      <td>1.286426</td>\n",
       "      <td>-0.591322</td>\n",
       "      <td>-0.666146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of message deleted</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>0.563455</td>\n",
       "      <td>-0.114554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for the update</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.101518</td>\n",
       "      <td>-0.202723</td>\n",
       "      <td>0.570407</td>\n",
       "      <td>-0.101533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on the moon</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>-0.345603</td>\n",
       "      <td>-0.381953</td>\n",
       "      <td>0.719988</td>\n",
       "      <td>-0.318832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>be with you</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.234381</td>\n",
       "      <td>-0.258666</td>\n",
       "      <td>-0.240453</td>\n",
       "      <td>0.553884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>how about cockroaches</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.130335</td>\n",
       "      <td>-0.201915</td>\n",
       "      <td>-0.174181</td>\n",
       "      <td>0.744809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ignorance is strength</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>-0.176142</td>\n",
       "      <td>-0.259921</td>\n",
       "      <td>-0.225740</td>\n",
       "      <td>0.866503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word(s)       pred_category  alt.atheism_coef  \\\n",
       "0           grow up childish         alt.atheism          0.626089   \n",
       "1              look up irony         alt.atheism          0.626089   \n",
       "2   up childish propagandist         alt.atheism          0.626089   \n",
       "3             agree with you       comp.graphics         -0.241578   \n",
       "4             am looking for       comp.graphics         -0.643466   \n",
       "5          thanks in advance       comp.graphics         -0.699582   \n",
       "6         of message deleted           sci.space         -0.118678   \n",
       "7             for the update           sci.space         -0.101518   \n",
       "8                on the moon           sci.space         -0.345603   \n",
       "9                be with you  talk.religion.misc         -0.234381   \n",
       "10     how about cockroaches  talk.religion.misc         -0.130335   \n",
       "11     ignorance is strength  talk.religion.misc         -0.176142   \n",
       "\n",
       "    comp.graphics_coef  sci.space_coef  talk.religion.misc_coef  \n",
       "0            -0.178170       -0.155396                -0.114554  \n",
       "1            -0.178170       -0.155396                -0.114554  \n",
       "2            -0.178170       -0.155396                -0.114554  \n",
       "3             0.666488       -0.378151                 0.129854  \n",
       "4             1.116126       -0.426317                -0.622128  \n",
       "5             1.286426       -0.591322                -0.666146  \n",
       "6            -0.178170        0.563455                -0.114554  \n",
       "7            -0.202723        0.570407                -0.101533  \n",
       "8            -0.381953        0.719988                -0.318832  \n",
       "9            -0.258666       -0.240453                 0.553884  \n",
       "10           -0.201915       -0.174181                 0.744809  \n",
       "11           -0.259921       -0.225740                 0.866503  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ngram in [(1,1), (2,2), (3,3)]:\n",
    "    ngram_vectorizer(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how we can improve the logistic regression by adjusting its regularization. Regularization can help us avoid learning too large of a weight for any single feature. There are two primary types for logistic regression: L1 (Lasso Regression) and L2 (Ridge Regression). The main difference between these two is that Lasso regression can bring the coefficient to zero, thus removing some features altogether. Ridge regression will not completely remove features. The default  regularization used for SciKit Learn's logistic regression is L2, which computes the coefficient size as the sum fo the squared weights.  \n",
    "  \n",
    "Let's explore using either L1 or L2 regularization on F1 score. Getting into some details: the default tolerance (tol) value is 0.0001, which means that the tolearnce for collinearity cannot exceed 0.0001. This is strict and can lead to convergence problems so it will be relaxed to 0.01. For the logistric regression macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularization type</th>\n",
       "      <th>number of non-zero weights</th>\n",
       "      <th>percent of coefficients that are non-zero</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1</td>\n",
       "      <td>2211</td>\n",
       "      <td>2</td>\n",
       "      <td>0.682414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L2</td>\n",
       "      <td>107516</td>\n",
       "      <td>100</td>\n",
       "      <td>0.673588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regularization type  number of non-zero weights  \\\n",
       "0                  L1                        2211   \n",
       "1                  L2                      107516   \n",
       "\n",
       "   percent of coefficients that are non-zero  F1 score  \n",
       "0                                          2  0.682414  \n",
       "1                                        100  0.673588  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tol = 0.01 \n",
    "\n",
    "# instantiating countvectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(train_data)\n",
    "X_dev = cv.transform(dev_data)\n",
    "\n",
    "# training LogisticRegression model using \"l1\" penalty\n",
    "lr_l1 = LogisticRegression(penalty=\"l1\", tol=tol)\n",
    "lr_l1.fit(X_train, train_labels)\n",
    "\n",
    "# training LogisticRegression model using \"l2\" penalty\n",
    "lr_l2 = LogisticRegression(penalty=\"l2\", tol=tol)\n",
    "lr_l2.fit(X_train, train_labels)\n",
    "\n",
    "def non_zero_weight_count(lr):\n",
    "    \"\"\"the number of learned weights that are not zero\"\"\"\n",
    "    count = 0\n",
    "    for weights in lr.coef_:\n",
    "        for weight in weights:\n",
    "            if weight != 0:\n",
    "                count += 1\n",
    "    return(count)\n",
    "def calc_F1(lr_model):\n",
    "    return metrics.f1_score(y_true=dev_labels, y_pred=lr_model.predict(X = X_dev), average='macro')\n",
    "\n",
    "coefficients_count = lr_l1.coef_.shape[0] * lr_l1.coef_.shape[1]\n",
    "\n",
    "summary_table = pd.DataFrame(\n",
    "    OrderedDict((\n",
    "        (\"regularization type\", ['L1', 'L2']),\n",
    "        (\"number of non-zero weights\", [non_zero_weight_count(lr_l1), non_zero_weight_count(lr_l2)]),\n",
    "     (\"percent of coefficients that are non-zero\", \\\n",
    "         [100 * non_zero_weight_count(lr_l1)/ coefficients_count,\n",
    "          100 * non_zero_weight_count(lr_l2)/ coefficients_count]),\n",
    "     (\"F1 score\", [calc_F1(lr_model) for lr_model in [lr_l1, lr_l2]])))\n",
    ")\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that L1 regularization set 99% of the coefficients to 0, but both have about the same F1 score. That in itself is pretty interesting. Considering the other 99% of coefficients to improve things much, they seem like calculation dead-weight, and I'm preferring the leanness of L1 regularization. Let's see what happens if we try and improve upon L1 and L2 regularization by testing out different lambda coefficients.  \n",
    "\n",
    "The lambda coefficient determines how aggresively to try and reduce coefficient magnitude. A large lambda says, try really hard to reduce the magnitude of coefficients, while a small lambda says, don't worry too much about coefficient magnitude. In SciKit Learn's logistic regression class, they refer to C which is simply 1/lambda. Thus, if C is large large coefficients are not as penalized as when C is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularization type</th>\n",
       "      <th>number of non-zero features</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>C hyperparameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.396846</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.624815</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L1</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.684870</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L1</td>\n",
       "      <td>2312.0</td>\n",
       "      <td>0.666484</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L1</td>\n",
       "      <td>6491.0</td>\n",
       "      <td>0.618008</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L1</td>\n",
       "      <td>20208.0</td>\n",
       "      <td>0.575365</td>\n",
       "      <td>1000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L1</td>\n",
       "      <td>25470.0</td>\n",
       "      <td>0.560051</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L1</td>\n",
       "      <td>25910.0</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>100000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L1</td>\n",
       "      <td>25966.0</td>\n",
       "      <td>0.541606</td>\n",
       "      <td>1000000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L2</td>\n",
       "      <td>26817.0</td>\n",
       "      <td>0.646087</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L2</td>\n",
       "      <td>26075.0</td>\n",
       "      <td>0.682685</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L2</td>\n",
       "      <td>26603.0</td>\n",
       "      <td>0.673588</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L2</td>\n",
       "      <td>26627.0</td>\n",
       "      <td>0.676057</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L2</td>\n",
       "      <td>26622.0</td>\n",
       "      <td>0.670355</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L2</td>\n",
       "      <td>26623.0</td>\n",
       "      <td>0.675971</td>\n",
       "      <td>1000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L2</td>\n",
       "      <td>26613.0</td>\n",
       "      <td>0.684205</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L2</td>\n",
       "      <td>26612.0</td>\n",
       "      <td>0.672083</td>\n",
       "      <td>100000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L2</td>\n",
       "      <td>26631.0</td>\n",
       "      <td>0.666084</td>\n",
       "      <td>1000000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regularization type  number of non-zero features  F1 score  \\\n",
       "0                   L1                         12.0  0.396846   \n",
       "1                   L1                        188.0  0.624815   \n",
       "2                   L1                        933.0  0.684870   \n",
       "3                   L1                       2312.0  0.666484   \n",
       "4                   L1                       6491.0  0.618008   \n",
       "5                   L1                      20208.0  0.575365   \n",
       "6                   L1                      25470.0  0.560051   \n",
       "7                   L1                      25910.0  0.575510   \n",
       "8                   L1                      25966.0  0.541606   \n",
       "9                   L2                      26817.0  0.646087   \n",
       "10                  L2                      26075.0  0.682685   \n",
       "11                  L2                      26603.0  0.673588   \n",
       "12                  L2                      26627.0  0.676057   \n",
       "13                  L2                      26622.0  0.670355   \n",
       "14                  L2                      26623.0  0.675971   \n",
       "15                  L2                      26613.0  0.684205   \n",
       "16                  L2                      26612.0  0.672083   \n",
       "17                  L2                      26631.0  0.666084   \n",
       "\n",
       "    C hyperparameter  \n",
       "0               0.01  \n",
       "1               0.10  \n",
       "2               1.00  \n",
       "3              10.00  \n",
       "4             100.00  \n",
       "5            1000.00  \n",
       "6           10000.00  \n",
       "7          100000.00  \n",
       "8         1000000.00  \n",
       "9               0.01  \n",
       "10              0.10  \n",
       "11              1.00  \n",
       "12             10.00  \n",
       "13            100.00  \n",
       "14           1000.00  \n",
       "15          10000.00  \n",
       "16         100000.00  \n",
       "17        1000000.00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nonzero_i(lr):\n",
    "    '''gets the nonzero indexes from a logistic regression ('lr') model'''\n",
    "    nonzero_i = []\n",
    "    for label in lr.coef_:\n",
    "        for i, weight in enumerate(label):\n",
    "            if weight > 0:\n",
    "                nonzero_i.append(i)\n",
    "    nonzero_i = list(set(nonzero_i)) # removing duplicates with set\n",
    "    return(nonzero_i)\n",
    "\n",
    "Cs = [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "F1s = []\n",
    "vocab_len = []\n",
    "summary_table = pd.DataFrame()\n",
    "for regularization_type in [\"l1\", \"l2\"]:\n",
    "    for C in Cs:\n",
    "        lr = LogisticRegression(penalty = regularization_type, \n",
    "                                tol = tol, \n",
    "                                C = C)\n",
    "        lr.fit(X_train, train_labels)   \n",
    "        \n",
    "        nonzero_i = find_nonzero_i(lr)        \n",
    "        pred = lr.predict(X = X_dev) # X_dev[:, nonzero_i]\n",
    "        F1 = metrics.f1_score(y_pred=pred, y_true=dev_labels, average = 'macro')\n",
    "        summary_table = summary_table.append(\n",
    "            {\n",
    "                \"regularization type\": regularization_type.capitalize(),\n",
    "                \"C hyperparameter\": C,\n",
    "                \"number of non-zero features\": len(nonzero_i),\n",
    "                \"F1 score\": F1\n",
    "            }, \n",
    "            ignore_index = True\n",
    "        )\n",
    "            \n",
    "summary_table[summary_table.columns[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that the C hyperparameter has a much greater effect on logistic regression using L1 regularization. When C is low, it means that the coefficient weight has a very big effect on the cost function, thus, only those features with very strong correlations are retained. The less impactful feature coefficients become zero. There is a sweet-spot, in this case around a C of 1, for which there are enough non-zero feature coefficients to produce the highest F1 score, after which, setting more coefficients to zero is detrimental. This is all in contrast to L2 regularization for which the C hyperparameter has no effect because coefficients cannot be set to zero. This of it like multiplying all coefficients by the same value. It doesn't change the outcome because it hasn't changed the nature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes for Topic Classification\n",
    "Let's now turn our attention now to Naive Bayes.\n",
    "The equation this will use is:  \n",
    "$$ p(topic\\ category\\ i\\ |\\ token\\ j) = \\frac{p(token\\ j\\ |\\ topic\\ category\\ i)p(topic\\ category\\ i)}{p(token\\ j)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.75      0.68       165\n",
      "          1       0.92      0.91      0.92       185\n",
      "          2       0.84      0.87      0.85       199\n",
      "          3       0.68      0.50      0.58       127\n",
      "\n",
      "avg / total       0.78      0.78      0.78       676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_dev = vectorizer.transform(dev_data)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X = X_train, y = train_labels)\n",
    "print classification_report(y_true = dev_labels, \n",
    "                      y_pred = nb.predict(X = X_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that Naive Bayes is performing better than logistic regression. One possible reason for the better performance of Naive Bayes is that Naive Bayes is less likely to overfit to the training data relative to logistic regression. To see if we can improve Naive Bayes further, let's attempt to improve its Laplacian smoothing hyperparameter. For scikit learn's multinomial Naive Bayes this is called alpha, and is the value added to all probabilities. A probability of zero for a feature, would completely eliminate the products of this zero probability feature, which can negatively impact prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcarrico/indeed/ishbooks-local/ishbook_env/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 score</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764591</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.765929</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.768043</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.770256</td>\n",
       "      <td>0.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.770484</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.771956</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.770714</td>\n",
       "      <td>0.155556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.767510</td>\n",
       "      <td>0.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.769409</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1 score     alpha\n",
       "0  0.363807  0.000000\n",
       "1  0.764591  0.022222\n",
       "2  0.765929  0.044444\n",
       "3  0.768043  0.066667\n",
       "4  0.770256  0.088889\n",
       "5  0.770484  0.111111\n",
       "6  0.771956  0.133333\n",
       "7  0.770714  0.155556\n",
       "8  0.767510  0.177778\n",
       "9  0.769409  0.200000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = list(np.linspace(0,0.2,10))\n",
    "summary_table = pd.DataFrame()\n",
    "for alpha in alphas:\n",
    "    mnb = MultinomialNB(alpha = alpha)\n",
    "    mnb.fit(X_train, train_labels)\n",
    "    dev_pred = mnb.predict(X = X_dev)\n",
    "    f1_score = metrics.f1_score(y_true = dev_labels,\n",
    "                               y_pred = dev_pred,\n",
    "                               average = \"macro\")\n",
    "    summary_table = summary_table.append({'alpha': alpha,\n",
    "                                         'F1 score': f1_score}, ignore_index = True)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These result show that a maximum F1 score is achieved for something near an alpha of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take our best performing conditions and test them with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized logistic regression conditions: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.31      0.41       154\n",
      "          1       0.43      0.92      0.59       204\n",
      "          2       0.56      0.41      0.47       195\n",
      "          3       0.64      0.11      0.19       124\n",
      "\n",
      "avg / total       0.55      0.49      0.44       677\n",
      "\n",
      "====================\n",
      "Optimized Multinomial Naive Bayes conditions: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.64      0.53       154\n",
      "          1       0.84      0.70      0.76       204\n",
      "          2       0.65      0.55      0.60       195\n",
      "          3       0.43      0.42      0.43       124\n",
      "\n",
      "avg / total       0.62      0.59      0.60       677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiating CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "# fitting and transforming training data\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "lr = LogisticRegression(tol = 0.01, penalty = \"l2\", C = 0.1)\n",
    "lr.fit(X = X_train, y = train_labels)\n",
    "print \"Optimized logistic regression conditions: \"\n",
    "print classification_report(y_true = test_labels, \n",
    "                      y_pred = lr.predict(X = X_test))\n",
    "print \"=\"*20\n",
    "mnb = MultinomialNB(alpha = 0.1)\n",
    "mnb.fit(X = X_train, y = train_labels)\n",
    "print \"Optimized Multinomial Naive Bayes conditions: \"\n",
    "print classification_report(y_true = test_labels, \n",
    "                      y_pred = mnb.predict(X = X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the conditions optimized for performance on the development data, Multinomial Naive Bayes does better than logisitc regression for all metrics on the test data. This may result from overfitting of training data by logistic regression. In summary, this shows us the power of the simple Naive Bayes classifier as applied to categorizing user posts into newsgroups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
